---
title: "Tidy methods for BART models using `{tidytreatment}`"
author: "Joshua J Bon"
institute: "CEREMADE, Universit√© Paris-Dauphine"
format:
  revealjs: 
    theme: default
editor: visual
---

## About me

- **Statistician** Develop statistical algorithms

- **Programmer** (Attempts) to write useful software

- **Data scientist** Design/implement statistical analyses

. . .

- PhD/PostDoc at Queensland Uni of Technology
- PostDoc at Paris-Dauphine PSL

## Talk overview

- ART: Additive Regression Tree
- B: Bayesian statistics
- Treatment effect models
- `{tidytreatment}`

# Additive regression trees

## Linear regression

```{r reg-setup}
library(ggplot2)
library(dplyr)
library(rpart)
library(rpart.plot)
set.seed(101)
x <- runif(30)
y <- x^2 + rnorm(length(x), sd = 0.1)
x <- x*100
y <- y*100
xydata <- tibble(x=x,y=y)
```

```{r linear-plot}

ggplot(xydata, aes(x=x,y=y)) +
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) +
  theme_bw()

```


## Linear regression

$$
y = \beta \cdot x + \varepsilon
$$

$$
\varepsilon \sim \text{N}(0,\sigma^2)
$$
When $p$ large, many variables and interaction to consider

- How to do model selection?
- What if relationship is not linear?


## Nonlinear regression

$$
y = \color{blue}{f(x; \beta)} + \varepsilon_i
$$

$$
\varepsilon \sim \text{N}(0,\sigma^2)
$$

- What to choose for $f$?
- Parsimonious and flexible

## What about binary trees?

```{mermaid}

graph TB
    A(( ))---B(( ))
    A---C(( ))
    B---D(( ))
    B---E(( ))
    C---F(( ))
    C---G(( ))
    D---H(( ))
    D---I(( ))
    E---J(( ))
    E---K(( ))
    F---L(( ))
    F---M(( ))
    G---N(( ))
    G---O(( ))

```

$$
\color{blue}{f(x; \beta)} = \text{piecewise constant}
$$

$$
\color{blue}{\beta} = \begin{cases}
T, & \text{Tree structure} \\
M, & \text{Terminal node values} \\
\end{cases}
$$

## Trees are flexible

```{r tree-plot-setup}

minvals <- c(30, 15, 10, 2)

```

:::: {.columns}

::: {.column width="50%"}

```{r pconstant-plot-1}

ggplot(xydata, aes(x=x,y=y)) +
  geom_point() + 
  geom_smooth(method = rpart, se = FALSE, 
              method.args = list(
                control = rpart.control(
                  minsplit = minvals[1],
                  minbucket = minvals[1]))) +
  theme_bw()

```


:::

::: {.column width="50%"}

```{r pconstant-tree-1}

model1 <- rpart(y ~ x, data=xydata, 
                     control = rpart.control(
                  minsplit = minvals[1],
                  minbucket = minvals[1]))

rpart.plot(model1)

```

:::

::::

## Trees are flexible

:::: {.columns}

::: {.column width="50%"}

```{r pconstant-plot-2}

ggplot(xydata, aes(x=x,y=y)) +
  geom_point() + 
  geom_smooth(method = rpart, se = FALSE, 
              method.args = list(
                control = rpart.control(
                  minsplit = minvals[2],
                  minbucket = minvals[2]))) +
  theme_bw()

```



:::

::: {.column width="50%"}

```{r pconstant-tree-2}

model2 <- rpart(y ~ x, data=xydata, 
                     control = rpart.control(
                  minsplit = minvals[2],
                  minbucket = minvals[2]))

rpart.plot(model2)

```

:::

::::

## Trees are flexible

:::: {.columns}

::: {.column width="50%"}

```{r pconstant-plot-3}

ggplot(xydata, aes(x=x,y=y)) +
  geom_point() + 
  geom_smooth(method = rpart, se = FALSE, 
              method.args = list(
                control = rpart.control(
                  minsplit = minvals[3],
                  minbucket = minvals[3]))) +
  theme_bw()

```

:::

::: {.column width="50%"}

```{r pconstant-tree-3}

model3 <- rpart(y ~ x, data=xydata, 
                     control = rpart.control(
                  minsplit = minvals[3],
                  minbucket = minvals[3]))

rpart.plot(model3)

```

:::

::::

## Trees are flexible

:::: {.columns}

::: {.column width="50%"}

```{r pconstant-plot-4}

ggplot(xydata, aes(x=x,y=y)) +
  geom_point() + 
  geom_smooth(method = rpart, se = FALSE, 
              method.args = list(
                control = rpart.control(
                  minsplit = minvals[4],
                  minbucket = minvals[4]))) +
  theme_bw()

```

:::

::: {.column width="50%"}

```{r pconstant-tree-4}

model4 <- rpart(y ~ x, data=xydata, 
                     control = rpart.control(
                  minsplit = minvals[4],
                  minbucket = minvals[4]))

rpart.plot(model4)

```

:::

::::

## Trees can model (non-linear) interactions

{{< pdf figs/regression-tree-diagram.pdf height=350 width=900 >}}

## Trees

More branches can result in

- Better approximation of smooth functions (same $x_i$)
- More interaction effects (different $x_i$)

## Trees

$$f(x; \beta) = g(x; T, M)$$

Unknown parameters in $\beta$:

- Terminal node mean values: $M = [\mu_1, \mu_2, \ldots, \mu_m]$
- Tree structure: $T$ (cut points, layers)


## Additive regression trees

$$f(x; \beta) = \sum_{k=1}^{K}g(x; T_k, M_k)$$

Unknown parameters in $\beta$:

- Terminal node mean values: $M_k$
- Tree structure: $T_k$ (cut points, layers)

## Additive regression trees

:::: {.columns}

::: {.column width="14%"}

$$\color{blue}{f(x;\beta)} =$$

:::

::: {.column width="18%"}
```{mermaid g1}
graph TB
    A(( ))---B(( ))
    A---C(( ))
    B---D(( ))
    B---E(( ))
    C---F(( ))
    C---G(( ))
```
:::

::: {.column width="8%"}

$$+$$
:::

::: {.column width="18%"}
```{mermaid g2}
graph TB
    J(( ))---K(( ))
    J---L(( ))
    L---M(( ))
    L---N(( ))
```
:::

::: {.column width="8%"}

$$+$$
:::

::: {.column width="8%"}

$$\cdots$$
:::

::: {.column width="8%"}

$$+$$

:::

::: {.column width="18%"}
```{mermaid g3}
graph TB
    A(( ))---B(( ))
    A---C(( ))
    C---F(( ))
    C---G(( ))
    G---H(( ))
    G---I(( ))
```
:::

::::


## Practical choices

- Shallow trees
- Larger number of trees $K>100$

# Bayesian statistics

## Parameters and priors

- Parameters are random variables with unknown value
- Represent uncertainty with probability distributions
- The **prior** distribution is our uncertainty before observing the data

## Observed data and likelihoods

- Observing data updates our prior distribution to the **posterior** distribution
- Information about the parameters is encoded through the likelihood

## Bayes rule and inference

$$
p(\theta ~\vert~ D) = \frac{p(D ~\vert~ \theta)p(\theta)}{p(D)}
$$

- E.g. $\theta = [\beta, \sigma^2]$
- Prior distribution $p(\theta)$
- Likelihood $p(D ~\vert~ \theta) = L(\theta~\vert~D)$
- Marginal likelihood $p(D)$
- Posterior distribution $p(\theta ~\vert~ D)$

## Computational inference

The posterior is often intractable. Typically the marginal likelihood $p(D)$ is not available in closed form.

Rely on Monte Carlo approximations using

$$
\theta_i \overset{\text{approx}}{\sim} p(\theta ~\vert~ D)
$$
for $i = 1, 2, \ldots, N$ for large $N$. $\theta_i$ is generated using MCMC or SMC

# Treatment effect models

## Causal modelling

Assumptions:

- Ignorability 
- Positivity $0 < \text{P}(A=1~\vert~X) < 1$ for all $X$

## Treatment effect BART

$$
y = \sum_{k=1}^{K}g(x, z; T_k, M_k) + \varepsilon
$$

- $z = 1$ is treatment group

## Bayesian causal forests

$$
y = \sum_{k=1}^{K}g(x; T_k, M_k) + z \sum_{k=1}^{K^\prime}g(x; T_k^\prime, M_k^\prime) + \varepsilon
$$

- $z = 1$ is treatment group


# `{tidytreatment}`

## Getting started

## Example

# Thanks

## Contact


- [joshuajbon@gmail.com](mailto:joshuajbon@gmail.com)

- <https://github.com/bonStats/tidytreatment>

- [@bonStats](https://twitter.com/bonStats)
